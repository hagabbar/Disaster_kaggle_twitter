{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T17:34:47.629974Z","iopub.execute_input":"2022-02-08T17:34:47.631082Z","iopub.status.idle":"2022-02-08T17:34:50.049651Z","shell.execute_reply.started":"2022-02-08T17:34:47.630982Z","shell.execute_reply":"2022-02-08T17:34:50.048147Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Load Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(r'/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv(r'/kaggle/input/nlp-getting-started/test.csv')\nsample_submission = pd.read_csv(r'/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:34:50.052127Z","iopub.execute_input":"2022-02-08T17:34:50.052533Z","iopub.status.idle":"2022-02-08T17:34:50.102688Z","shell.execute_reply.started":"2022-02-08T17:34:50.052436Z","shell.execute_reply":"2022-02-08T17:34:50.101597Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#sample_submission.keys()\ntrain_df.keys()\n#test_data.keys()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:34:50.105954Z","iopub.execute_input":"2022-02-08T17:34:50.106202Z","iopub.status.idle":"2022-02-08T17:34:50.120959Z","shell.execute_reply.started":"2022-02-08T17:34:50.106172Z","shell.execute_reply":"2022-02-08T17:34:50.119809Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"A Quick Look at the Data","metadata":{}},{"cell_type":"code","source":"train_df[train_df[\"target\"] == 0][\"text\"].values[1]\ntrain_df[train_df[\"target\"] == 1][\"text\"].values[1]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:34:50.123903Z","iopub.execute_input":"2022-02-08T17:34:50.125125Z","iopub.status.idle":"2022-02-08T17:34:50.136592Z","shell.execute_reply.started":"2022-02-08T17:34:50.125079Z","shell.execute_reply":"2022-02-08T17:34:50.135275Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Transform data into vector","metadata":{}},{"cell_type":"code","source":"count_vectorizer = feature_extraction.text.CountVectorizer()\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:34:50.138107Z","iopub.execute_input":"2022-02-08T17:34:50.138600Z","iopub.status.idle":"2022-02-08T17:34:50.147310Z","shell.execute_reply.started":"2022-02-08T17:34:50.138462Z","shell.execute_reply":"2022-02-08T17:34:50.146054Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:34:50.149584Z","iopub.execute_input":"2022-02-08T17:34:50.150221Z","iopub.status.idle":"2022-02-08T17:34:50.160857Z","shell.execute_reply.started":"2022-02-08T17:34:50.150161Z","shell.execute_reply":"2022-02-08T17:34:50.159454Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Convert all text in training and test set to vectors","metadata":{}},{"cell_type":"code","source":"train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:34:50.162978Z","iopub.execute_input":"2022-02-08T17:34:50.163944Z","iopub.status.idle":"2022-02-08T17:34:50.453267Z","shell.execute_reply.started":"2022-02-08T17:34:50.163891Z","shell.execute_reply":"2022-02-08T17:34:50.452199Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_vectors[0].todense()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:34:50.455274Z","iopub.execute_input":"2022-02-08T17:34:50.455636Z","iopub.status.idle":"2022-02-08T17:34:50.466778Z","shell.execute_reply.started":"2022-02-08T17:34:50.455589Z","shell.execute_reply":"2022-02-08T17:34:50.465731Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**TensorFlow Approach**\n","metadata":{}},{"cell_type":"code","source":"# Convert Data to array\ntrain_vectors = train_vectors.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:34:50.468557Z","iopub.execute_input":"2022-02-08T17:34:50.468998Z","iopub.status.idle":"2022-02-08T17:34:50.687717Z","shell.execute_reply.started":"2022-02-08T17:34:50.468950Z","shell.execute_reply":"2022-02-08T17:34:50.686679Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = keras.Sequential()\n# Add an Embedding layer expecting input vocab of size 1000, and\n# output embedding dimension of size 64.\nmodel.add(layers.Embedding(input_dim=21637, output_dim=64))\n\n# Add a LSTM layer with 128 internal units.\nmodel.add(layers.LSTM(256))\n\n# Add a Dense layer with 10 units.\nmodel.add(layers.Dense(1))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:35:37.110253Z","iopub.execute_input":"2022-02-08T17:35:37.111213Z","iopub.status.idle":"2022-02-08T17:35:37.424333Z","shell.execute_reply.started":"2022-02-08T17:35:37.111171Z","shell.execute_reply":"2022-02-08T17:35:37.420935Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Compile the model","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy(),\n                       tf.keras.metrics.FalseNegatives()])","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:35:41.448357Z","iopub.execute_input":"2022-02-08T17:35:41.448798Z","iopub.status.idle":"2022-02-08T17:35:41.484853Z","shell.execute_reply.started":"2022-02-08T17:35:41.448754Z","shell.execute_reply":"2022-02-08T17:35:41.483632Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Fit the model","metadata":{}},{"cell_type":"code","source":"model.fit(train_vectors, train_df[\"target\"], epochs=1, batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:35:43.359814Z","iopub.execute_input":"2022-02-08T17:35:43.360086Z","iopub.status.idle":"2022-02-08T17:35:58.229927Z","shell.execute_reply.started":"2022-02-08T17:35:43.360055Z","shell.execute_reply":"2022-02-08T17:35:58.228269Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"] = clf.predict(test_vectors)\nsample_submission.head()\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sklearn approach**","metadata":{}},{"cell_type":"code","source":"## Our vectors are really big, so we want to push our model's weights\n## toward 0 without completely discounting different words - ridge regression \n## is a good way to do this.\n#clf = linear_model.RidgeClassifier()\n#from sklearn.neural_network import MLPClassifier\n#clf = MLPClassifier(hidden_layer_sizes=10, max_iter=400)\n#from sklearn.svm import SVC\n#clf = SVC()\n#from sklearn.ensemble import RandomForestClassifier\n#clf = RandomForestClassifier()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train and test our model","metadata":{}},{"cell_type":"code","source":"#scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\", verbose=3)\n#scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict on training set and start building a submission for the competition.","metadata":{}},{"cell_type":"code","source":"#print(train_vectors.shape)\n#print(train_df[\"target\"].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clf.fit(train_vectors, train_df[\"target\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample_submission[\"target\"] = clf.predict(test_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}